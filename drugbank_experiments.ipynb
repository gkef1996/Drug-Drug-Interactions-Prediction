{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import gzip\n",
    "import collections\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pubchempy as pcp\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import SDMolSupplier\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randrange, choice\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KL Divergence Computation\n",
    "import numpy as np\n",
    "\n",
    "def KL_aux(a, b):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "\n",
    "    return np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
    "\n",
    "#### KL Divergence for incompatible distributions (input: Series for each dataset)\n",
    "def KL(x,y):\n",
    "    x = x.value_counts().sort_index()\n",
    "    y = y.value_counts().sort_index()\n",
    "    index = x.index.union(y.index).unique()\n",
    "    x = x.reindex(index).fillna(1)\n",
    "    y = y.reindex(index).fillna(1)\n",
    "    x = x/x.values.sum()\n",
    "    y = y/y.values.sum()\n",
    "    return KL_aux(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DrugBank Dataset XML extraction\n",
    "def get_text(element, tag):\n",
    "    e = element.find(tag)\n",
    "    if e is not None:\n",
    "        return e.text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "database = open(\"../Datasets/DrugBank/full database.xml\", 'r') ### Replace with correct path to DrugBank Dataset XML file\n",
    "tree = ET.parse(database)\n",
    "root = tree.getroot()\n",
    "print(\"Total drug entities: {}\".format(len(root)))\n",
    "\n",
    "### Read Structures from SDF format\n",
    "structures_df = PandasTools.LoadSDF(\"../Datasets/DrugBank/structures.sdf\")\n",
    "print(\"Total drugs on Stuctures (MOL) supplementary file: {}\".format(structures_df.shape[0]))\n",
    "\n",
    "### Read Structures in CSV Format (fewer columns)\n",
    "#structures_df = pd.read_csv(\"../Datasets/DrugBank/structure links.csv\")  \n",
    "\n",
    "### Add mol2vec vectors to structures_df\n",
    "#Loading pre-trained model via word2vec\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load('model_300dim.pkl')\n",
    "\n",
    "from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "\n",
    "# Compute mol2vec vectors for all smiles \n",
    "print(\"Computing mol2vec embeddings for all available MOL structures\")\n",
    "#Constructing sentences\n",
    "structures_df['sentence'] = structures_df.apply(lambda x: MolSentence(mol2alt_sentence(x['ROMol'], 1)), axis=1)\n",
    "\n",
    "#Extracting embeddings to a numpy.array\n",
    "#Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\n",
    "structures_df['mol2vec'] = [DfVec(x) for x in sentences2vec(structures_df['sentence'], model, unseen='UNK')]\n",
    "# Keep track of Drugbank IDs for which we have MOL structure data\n",
    "valid_ids = set(structures_df.DRUGBANK_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse DrugBank Database to find interactions\n",
    "total_drugs = 0\n",
    "interactions_dict = {}\n",
    "groups_dict = {}\n",
    "name_dict = {}\n",
    "kingdom_dict = {}\n",
    "superclass_dict = {}\n",
    "categories_dict = {}\n",
    "for drug in tqdm(root, total = len(root)):\n",
    "    total_drugs += 1\n",
    "\n",
    "     #---------drugbank-id --------\n",
    "    dbid = drug.find(\"{http://www.drugbank.ca}drugbank-id\")\n",
    "    #print(dbid.text)\n",
    "    current_id = dbid.text\n",
    "\n",
    "    name = drug.find(\"{http://www.drugbank.ca}name\")\n",
    "    if name is not None:\n",
    "        #print(name.text)\n",
    "        name_dict[current_id] = name.text\n",
    "\n",
    "    #-----------------Interactions -----\n",
    "    interactions = drug.find(\"{http://www.drugbank.ca}drug-interactions\")\n",
    "    \n",
    "    interaction_list = []\n",
    "    for t in interactions:\n",
    "        i = t.find(\"{http://www.drugbank.ca}drugbank-id\")\n",
    "        #print(i.text)\n",
    "        interaction_list.append(i.text)\n",
    "\n",
    "    interactions_dict[current_id] = interaction_list\n",
    "\n",
    "     #-----------------External Identifiers -----\n",
    "    ext_identifiers = drug.find(\"{http://www.drugbank.ca}external-identifiers\")\n",
    "    \n",
    "    interaction_list = []\n",
    "    for t in interactions:\n",
    "        i = t.find(\"{http://www.drugbank.ca}drugbank-id\")\n",
    "        #print(i.text)\n",
    "        interaction_list.append(i.text)\n",
    "\n",
    "    #------------------Drug Classification\n",
    "    classification = drug.find(\"{http://www.drugbank.ca}classification\")\n",
    "    if classification is not None:\n",
    "        kingdom = classification.find(\"{http://www.drugbank.ca}kingdom\").text\n",
    "        kingdom_dict[current_id] = kingdom\n",
    "        superclass = classification.find(\"{http://www.drugbank.ca}superclass\").text\n",
    "        superclass_dict[current_id] = superclass\n",
    "    \n",
    "    #-----------------Drug Category (Amino Acids)\n",
    "    categories = drug.find(\"{http://www.drugbank.ca}categories\")\n",
    "    if categories is not None:\n",
    "        category = categories.find(\"{http://www.drugbank.ca}category\")\n",
    "        if category is not None:\n",
    "            categories_string = category.find(\"{http://www.drugbank.ca}category\").text\n",
    "            categories_dict[current_id] = list(map(lambda x : x if (\"and \" not in x) else x.split('and ')[1], categories_string.split(';')))\n",
    "    \n",
    "    #----------------Drug Groups (e.g. Approved)\n",
    "    groups = drug.find(\"{http://www.drugbank.ca}groups\")\n",
    "    if groups is not None:\n",
    "        groups = groups.find(\"{http://www.drugbank.ca}group\").text.split(';')\n",
    "        groups_dict[current_id] = groups[0]\n",
    "        if len(groups) > 1 :\n",
    "            print(current_id) # check if any drug belongs to more than one group\n",
    "            break\n",
    "\n",
    "### Create Dataframes for drugs and interactions\n",
    "drugs_df = pd.DataFrame(name_dict.items(), columns=['DrugbankId', 'Name'])\n",
    "drugs_df['Group'] = drugs_df.DrugbankId.map(groups_dict)\n",
    "drugs_df['Kingdom'] = drugs_df.DrugbankId.map(kingdom_dict)\n",
    "drugs_df['Superclass'] = drugs_df.DrugbankId.map(superclass_dict)\n",
    "drugs_df['Categories'] = drugs_df.DrugbankId.map(categories_dict)\n",
    "interactions_df = pd.DataFrame(interactions_dict.items(), columns=['SDID', 'ODID'])\n",
    "interactions_df = interactions_df.explode('ODID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set of unsupported DIDs:\n",
    "interaction_dids = set(interactions_df.SDID.unique())\n",
    "unsupported_ids = interaction_dids.difference(valid_ids)\n",
    "print(\"Unsupported IDs: {}\".format(len(unsupported_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "did2smiles = {}\n",
    "### Use PubChem for each unsupported ID\n",
    "for uid in tqdm(unsupported_ids, total = len(unsupported_ids)):\n",
    "    res_list = pcp.get_compounds(name_dict[uid], 'name')\n",
    "    if len(res_list) == 0: \n",
    "        continue\n",
    "    did2smiles[uid] = res_list[0].isomeric_smiles\n",
    "print(\"Fetched SMILES from PubChemPy: {} / {}\".format(len(did2smiles.keys()), len(unsupported_ids)))\n",
    "### Append new SMILES to structures_df\n",
    "new_structures = pd.DataFrame(did2smiles.items(), columns=['DATABASE_ID', 'SMILES'])\n",
    "new_structures['ROMol'] = new_structures['SMILES'].apply(lambda x: MolFromSmiles(x))\n",
    "if new_structures[new_structures['ROMol'].isnull()].shape[0] > 0:\n",
    "    print('Warining: could not convert the following SMILES to Chm.Mol: \\n{}'.format(new_structures[new_structures['ROMol'].isnull()]))\n",
    "    print(\"Droping substances that were not converted.\")\n",
    "    new_structures = new_structures.dropna()\n",
    "structures_df = structures_df.append(new_structures)\n",
    "### Update unsupported IDs\n",
    "valid_ids = set(structures_df.DATABASE_ID.unique())\n",
    "unsupported_ids = interaction_dids.difference(valid_ids)\n",
    "### Compute Mol2Vec for all structures again (to cover new entries from PubChem)\n",
    "structures_df['sentence'] = structures_df.apply(lambda x: MolSentence(mol2alt_sentence(x['ROMol'], 1)), axis=1)\n",
    "structures_df['mol2vec'] = [DfVec(x) for x in sentences2vec(structures_df['sentence'], model, unseen='UNK')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Organizing Available Data\n",
    "print(\"Valid IDs with SMILES\")\n",
    "# Remove from interactions dataframe unsupported ids\n",
    "valid_interactions = interactions_df[interactions_df['SDID'].isin(valid_ids)]\n",
    "valid_interactions = valid_interactions[valid_interactions['ODID'].isin(valid_ids)]\n",
    "print(\"Total Valid Interactions: {} (from original {} interactions)\".format(valid_interactions.shape[0], interactions_df.shape[0]))\n",
    "print(\"Total Valid Drug IDs: {}\".format(len(valid_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Mol2Vec Experiment Setup\n",
    "\n",
    "#### Prepare a dataframe for the classification problem {col1: features_vector, col2: call_value}. \n",
    "#### features vector: concatenated features_vectors of two substances. Values: 0-->do not interact, 1-->interact\n",
    "id2mol2vec = dict(zip(structures_df.DATABASE_ID, structures_df.mol2vec))\n",
    "classification_df = pd.DataFrame({'features_vector':valid_interactions.apply(lambda r: [str(r.SDID), str(r.ODID)], axis = 1)})\n",
    "mask = [(x[0] < x[1]) for x in classification_df['features_vector']]\n",
    "classification_df = classification_df[mask] # Keep only one relation per drug interaction (A-->B, discard B-->A)\n",
    "classification_df['class'] = 1 \n",
    "classification_df['SDID'] =  classification_df.apply(lambda r: r.features_vector[0], axis=1)\n",
    "classification_df['ODID'] =  classification_df.apply(lambda r: r.features_vector[1], axis=1)\n",
    "classification_df['features_vector'] = classification_df.apply(lambda r: np.concatenate([id2mol2vec[r.features_vector[0]].vec, id2mol2vec[r.features_vector[1]].vec]), axis=1)\n",
    "print(\"Classification DF shape:{}\".format(classification_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE Data from Pickles --- folder: drugbank_experiment_pickles\n",
    "# with open('drugbank_experiment_pickles/structures_df.pickle', 'wb') as handle:\n",
    "#     pickle.dump(structures_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('drugbank_experiment_pickles/valid_interactions.pickle', 'wb') as handle:\n",
    "#     pickle.dump(valid_interactions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('drugbank_experiment_pickles/classification_df.pickle', 'wb') as handle:\n",
    "#     pickle.dump(classification_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('drugbank_experiment_pickles/valid_ids.pickle', 'wb') as handle:\n",
    "#     pickle.dump(valid_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('drugbank_edge_betweeness_centrality.pickle', 'wb') as handle:  # Store betweeness centrality of graph\n",
    "#     pickle.dump(edge_betweeness_centrality, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('drugbank_groups_dict_centrality.pickle', 'wb') as handle:  # Store betweeness centrality of graph\n",
    "#     pickle.dump(groups_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "### LOAD Data from Pickles --- folder: drugbank_experiment_pickless\n",
    "\n",
    "with open('drugbank_experiment_pickles/structures_df.pickle', 'rb') as handle:\n",
    "    structures_df = pickle.load(handle)\n",
    "\n",
    "with open('drugbank_experiment_pickles/valid_interactions.pickle', 'rb') as handle:\n",
    "    valid_interactions = pickle.load(handle)\n",
    "\n",
    "with open('drugbank_experiment_pickles/classification_df.pickle', 'rb') as handle:\n",
    "    classification_df = pickle.load(handle)\n",
    "\n",
    "with open('drugbank_experiment_pickles/valid_ids.pickle', 'rb') as handle:\n",
    "    valid_ids = pickle.load(handle)\n",
    "    \n",
    "with open('drugbank_edge_betweeness_centrality.pickle', 'rb') as handle:\n",
    "    edge_betweeness_centrality = pickle.load(handle) \n",
    "\n",
    "with open('drugbank_groups_dict_centrality.pickle', 'rb') as handle:\n",
    "    groups_dict = pickle.load(handle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Negative Samples\n",
    "# Add samples with substances that do not interact\n",
    "# Generate random tuples from the substances list, and if they do not appear on the interactions_df add them as a non interaction\n",
    "\n",
    "def getSetWithAllPairs(df, col1, col2):\n",
    "    return set(list(zip(df[col1],df[col2])))\n",
    " \n",
    "def getNegSamples(pairs_to_avoid, substances_df, neg_sample_size, result_list, result_idx, bad_cids):\n",
    "    subs_did_list = list(set(valid_interactions.SDID.unique()).union(valid_interactions.ODID.unique()))\n",
    "    neg_examples = []\n",
    "    while len(neg_examples) < neg_sample_size:\n",
    "        # Generate random pair\n",
    "        did_a = choice(subs_did_list)\n",
    "        did_b = choice(subs_did_list)\n",
    "        if((did_a in bad_cids) or (did_b in bad_cids)):\n",
    "            continue\n",
    "        flag1 = (did_a, did_b) not in pairs_to_avoid\n",
    "        flag2 = (did_b, did_a) not in pairs_to_avoid\n",
    "        if(did_a != did_b and flag1 and flag2):\n",
    "            try:   \n",
    "                neg_examples.append([did_a, did_b, 0])\n",
    "            except IndexError:\n",
    "                print(\"IndexError for subject: {} and object: {}\".format(did_a, did_b))\n",
    "    result_list[result_idx-1] = neg_examples\n",
    "    return\n",
    "\n",
    "thread_list = []\n",
    "NEG_SAMPLE_SIZE = 350000 # For each thread\n",
    "THREADS = 4\n",
    "result_list = [None]*THREADS\n",
    "pairs_2_avoid = getSetWithAllPairs(valid_interactions, 'SDID', 'ODID')\n",
    "bad_cids = {}\n",
    "print(\"Initiating Multithreading Execution\")\n",
    "for i in range(THREADS):\n",
    "    thread = Thread(target = getNegSamples, args = [pairs_2_avoid,structures_df, NEG_SAMPLE_SIZE, result_list, i, bad_cids])\n",
    "    thread_list.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for t in thread_list:\n",
    "    t.join()\n",
    "\n",
    "print(\"Exited Multithreaded Section\")\n",
    "\n",
    "neg_examples = []\n",
    "for l in result_list:\n",
    "    if l != None: neg_examples.extend(l)\n",
    "            \n",
    "cid2mol2vec = dict(zip(structures_df.DATABASE_ID, structures_df.mol2vec))\n",
    "print(\"Creating Dataframe with negative samples\")\n",
    "neg_examples_df = pd.DataFrame(data = neg_examples, columns=['SDID', 'ODID', 'class'])\n",
    "print(\"Removing Duplicate vectors\") \n",
    "#Remove duplicates\n",
    "neg_examples_df = neg_examples_df.drop_duplicates()\n",
    "\n",
    "# Create features_vector column\n",
    "print(\"Applying mapp to feature vectors\")\n",
    "neg_examples_df[\"features_vector\"] = neg_examples_df.apply(lambda r: np.concatenate([cid2mol2vec[r.SDID].vec, cid2mol2vec[r.ODID].vec]), axis = 1)\n",
    "mol2vec_neg_examples_df = neg_examples_df # store negative examples for cross evaluating with node2vec experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Test Dataframe with positive and negative examples of interactions\n",
    "test_dataframe = pd.concat([classification_df, neg_examples_df], axis=0)\n",
    "print(\"Shape of Test Dataframe: {}\".format(test_dataframe.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Starting the Classification Experiments #########################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Function to recover bidirectional interactions\n",
    "def addReverseVector(df):\n",
    "    #m = (df['class'] == 1) #uncomment to add reverse vectors only for one class\n",
    "    #df1 = df[m].copy()\n",
    "    df1 = df.copy()\n",
    "    df1['features_vector'] = df1.features_vector.apply(lambda x: np.concatenate([x[:len(x)//2], x[len(x)//2:]]))\n",
    "    return  pd.concat([df, df1], axis=0)\n",
    "\n",
    "# Separate features vector from class\n",
    "test_df = test_dataframe[['features_vector', 'SDID', 'ODID', 'class']]\n",
    "X = test_df.iloc[:, 0:-1]\n",
    "y = test_df.iloc[:, -1]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=69)\n",
    "trainDF = addReverseVector(pd.concat([X_train,y_train], axis=1))\n",
    "X_train, y_train= trainDF.features_vector, trainDF['class']\n",
    "testDF = addReverseVector(pd.concat([X_test,y_test], axis=1))\n",
    "X_test, y_test, test_pairs = testDF.features_vector, testDF['class'],  testDF[['SDID', 'ODID']]\n",
    "valDF = addReverseVector(pd.concat([X_val,y_val], axis=1))\n",
    "X_val, y_val= valDF.features_vector, valDF['class']\n",
    "\n",
    "print(\"Shape of X_train: {} ------ Class_0: {} -- Class_1: {}\".format(X_train.shape,  y_train[(y_train.values==0)].count(), y_train[(y_train.values==1)].count()))\n",
    "print(\"Shape of X_test: {}  ------ Class_0: {} -- Class_1: {}\".format(X_test.shape, y_test[(y_test.values==0)].count(), y_test[(y_test.values==1)].count()))\n",
    "print(\"Shape of X_val: {}  ------ Class_0: {}  -- Class_1: {}\".format(X_val.shape, y_val[(y_val.values==0)].count(), y_val[(y_val.values==1)].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting counts for Train Data\")\n",
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_train, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting counts for Validation Data\")\n",
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_val, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting counts for Test Data\")\n",
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_test, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataclass TrainData(Dataset):\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train.values.tolist()), \n",
    "                       torch.FloatTensor(y_train.values.tolist()))\n",
    "## test data    \n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "test_data = TestData(torch.FloatTensor(X_test.values.tolist()))\n",
    "val_data = TestData(torch.FloatTensor(X_val.values.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()        # Number of input features is 600.\n",
    "        self.layer_1 = nn.Linear(600, 1024)\n",
    "        self.layer_2 = nn.Linear(1024, 512)\n",
    "        self.layer_3 = nn.Linear(512, 256)\n",
    "        self.layer_out = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BinaryClassificationLarge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationLarge, self).__init__()        # Number of input features is 600.\n",
    "        self.layer_1 = nn.Linear(600, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 1024)\n",
    "        self.layer_3 = nn.Linear(1024, 512)\n",
    "        self.layer_4 = nn.Linear(512, 256)\n",
    "        self.layer_out = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(2048)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_4(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class BinaryClassificationFat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationFat, self).__init__()        # Number of input features is 600.\n",
    "        self.layer_1 = nn.Linear(600, 4096)\n",
    "        self.layer_2 = nn.Linear(4096, 2048)\n",
    "        self.layer_out = nn.Linear(2048, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(4096)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(2048)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "# Define Model\n",
    "model = BinaryClassification()\n",
    "model.cuda()\n",
    "print(model)\n",
    "print(\"Model Size: \", model.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Train the model\n",
    "# Model Parameters\n",
    "EPOCHS = 70\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING = 5\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bad_progress_counter = 0\n",
    "model_epoch = 0\n",
    "best_val_acc = 0\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=1)\n",
    "print(device)\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    if e > 5:\n",
    "        BATCH_SIZE = 32\n",
    "        LEARNING_RATE = 0.0005\n",
    "    elif e > 60:\n",
    "        BATCH_SIZE = 16\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()  \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val_pred = model(X_batch)\n",
    "            y_val_pred = torch.sigmoid(y_val_pred)\n",
    "            y_pred_tag = torch.round(y_val_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    val_acc = accuracy_score(y_pred_list, y_val)\n",
    "    if e==1: \n",
    "        best_val_acc = val_acc\n",
    "    elif best_val_acc <= val_acc:\n",
    "        bad_progress_counter = 0\n",
    "    else:\n",
    "        bad_progress_counter +=1\n",
    "    \n",
    "    if bad_progress_counter == EARLY_STOPPING:\n",
    "        print(\"Applying Early Stop\")\n",
    "        break\n",
    "    # End of validation step\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Val Acc: {val_acc}')\n",
    "    if(val_acc > best_val_acc):\n",
    "        mol2vec_model = model   #Store model\n",
    "        model_epoch = e\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "print(\"Keeping model from epoch: {}\".format(model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model\n",
    "model = mol2vec_model\n",
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, y_pred_list)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE Result Data\n",
    "# with open('drugbank_experiment_pickles/mol2vec_large2_full_results.pickle', 'wb') as handle:\n",
    "#     pickle.dump((test_pairs, y_test, y_pred_list), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### LOAD Data from Pickles --- folder: drugbank_experiment_pickless\n",
    "with open('drugbank_experiment_pickles/mol2vec_large2_full_results.pickle', 'rb') as handle:\n",
    "    test_pairs_m, y_test_m, y_pred_list_m = pickle.load(handle)\n",
    "test_pairs, y_test, y_pred_list = test_pairs_m, y_test_m, y_pred_list_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "###### Compute Useful Test Results regadring Graph Properites on the Test Set's Nodes\n",
    "# test_pairs, degreeNodes, y_test, y_pred\n",
    "correct_degrees, min_correct_degrees, max_correct_degrees = [], [], []\n",
    "false_degrees, min_false_degrees, max_false_degrees = [], [], []\n",
    "correct_core_diffs, false_core_diffs = [], []\n",
    "correct_b_centrality, false_b_centrality = [], []\n",
    "pairs = test_pairs.values\n",
    "test_values = y_test.values\n",
    "for i in range(len(pairs)):\n",
    "    scid, ocid = pairs[i][0], pairs[i][1]\n",
    "    sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "    score, ocore = core_dict[scid], core_dict[ocid]\n",
    "    if int(test_values[i]) == int(y_pred_list[i]):   # Correct decision for NN\n",
    "        correct_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_correct_degrees.append(min(sdegree, odegree))\n",
    "        max_correct_degrees.append(max(sdegree, odegree))\n",
    "        correct_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            correct_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        false_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_false_degrees.append(min(sdegree, odegree))\n",
    "        max_false_degrees.append(max(sdegree, odegree))\n",
    "        false_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            false_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(\"Mean Correct Degrees: {}\".format(mean(correct_degrees)))\n",
    "print(\"Mean MIN Correct Degrees: {}\".format(mean(min_correct_degrees)))\n",
    "print(\"Mean MAX Correct Degrees: {}\".format(mean(max_correct_degrees)))\n",
    "print(\"Mean Correct Core Differences: {}\".format(mean(correct_core_diffs)))\n",
    "print(\"Mean Correct Betweenus Centrality: {}\".format(mean(correct_b_centrality)))\n",
    "print(\"Mean False Degrees: {}\".format(mean(false_degrees)))\n",
    "print(\"Mean MIN False Degrees: {}\".format(mean(min_false_degrees)))\n",
    "print(\"Mean MAX False Degrees: {}\".format(mean(max_false_degrees)))\n",
    "print(\"Mean False Core Differences: {}\".format(mean(false_core_diffs)))\n",
    "print(\"Mean False Betweenus Centrality: {}\".format(mean(false_b_centrality)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for Plots\n",
    "resolution = 1000\n",
    "b_centrality_limit = 3\n",
    "distribution_dict = {}\n",
    "for i in tqdm(range(len(pairs))):\n",
    "   scid, ocid = pairs[i][0], pairs[i][1]\n",
    "   sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "   score, ocore = core_dict[scid], core_dict[ocid]\n",
    "   is_positive =  int(test_values[i])\n",
    "   correctly_identified = int(test_values[i]) == int(y_pred_list[i])\n",
    "   mean_degree = 1.0*(sdegree + odegree)/2\n",
    "   min_degree = min(sdegree, odegree)\n",
    "   max_degree = max(sdegree, odegree)\n",
    "   core_diff = abs(score-ocore)\n",
    "   try:\n",
    "      b_centrallity = 10**6*edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)]\n",
    "   except: \n",
    "      b_centrallity = None\n",
    "   distribution_dict[(scid, ocid)] = [is_positive, correctly_identified, mean_degree, min_degree, max_degree, core_diff, b_centrallity]\n",
    "\n",
    "distribution_df = pd.DataFrame.from_dict(data = distribution_dict, orient = 'index' , columns= ['is_positive', 'correctly_identified', 'mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters for results plots file save\n",
    "resultsPath = \"result_plots/large_drugbank_results/mol2vec/large_nn/\"   ### CHANGE THIS FOR EVERY EXPERIMENT\n",
    "metrics_dict = {\"mean_degree\":\"Mean Degree\", \"min_degree\":\"Min Degree\", \"max_degree\":\"Max Degree\", \"core_diff\":\"Core Difference\", \"b_centrality\":\"Betweenness Centrality\"}\n",
    "\n",
    "### Plot Group A: Histogram grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=False, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Frequency Histogram of Classifier's predictions\", ylabel='Frequency', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupA_Frequency Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group AX: Density grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    # Normalize\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Density Histogram of Classifier's predictions\", ylabel='Density', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupAX_Density Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "   \n",
    "### Plot Group B: Density grouped by categories in separate subplots\n",
    "\n",
    "param_list = []\n",
    "for pos in [0,1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "colors = ['tab:green', 'tab:blue', 'tab:red', 'tab:cyan']\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "param_list = []\n",
    "for pos in [1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['b_centrality']:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        x = x[x.between(0,b_centrality_limit)]\n",
    "\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group C: KDE \n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    dv_list = []\n",
    "    i=0\n",
    "    for pos in [0,1]:\n",
    "        for corr in [True, False]:\n",
    "            dv = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)].value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs, bw_method=0.03)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "for selector in ['b_centrality']:\n",
    "    dv_list = []\n",
    "    i=2\n",
    "    for pos in [1]:\n",
    "        for corr in [True, False]:\n",
    "            x = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)]\n",
    "            x = x[x.between(0,b_centrality_limit)]\n",
    "            dv = x.value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "### Compute Stats\n",
    "print(\"Total Negative Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Negative Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "print(\"Total Positive Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Positive Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "\n",
    "#### B-centrality-stats\n",
    "print(\"\\nB centrality plots where limited on samples with b_centrality < {}\".format(b_centrality_limit))\n",
    "print(\"Total nodes with b_centrality <  {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality < b_centrality_limit)].shape[0]))\n",
    "print(\"Total nodes with b_centrality >= {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality >= b_centrality_limit)].shape[0]))\n",
    "\n",
    "\n",
    "print(\"\\nKL Divergence Results\")\n",
    "### KL Divergence Computation\n",
    "print(\"KL Divergence between distributions from Classifier choices\")\n",
    "kl_list = []\n",
    "for index, selector in enumerate(['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']):\n",
    "    print(\"---\" + selector + \" ~ KL(Correct | False): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==True][selector], distribution_df[distribution_df.correctly_identified==False][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(False | Correct): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==False][selector], distribution_df[distribution_df.correctly_identified==True][selector])))\n",
    "\n",
    "####### Evaluate Model's Predictions on Drug Groups\n",
    "distribution_df['sgroup'] = distribution_df.index.map(lambda x: groups_dict[x[0]])\n",
    "distribution_df['ogroup'] =distribution_df.index.map(lambda x: groups_dict[x[1]])\n",
    "\n",
    "# Print (ordered) Accuracy and total samples for each category (when at least one drug on the interaction belongs to a category)\n",
    "group_names = ['approved', 'investigational', 'experimental', 'illicit', 'vet_approved', 'withdrawn', 'nutraceutical']\n",
    "\n",
    "for name in group_names:\n",
    "    temp_df = distribution_df[(distribution_df.sgroup==name) | (distribution_df.ogroup==name)]\n",
    "    accuracy = temp_df[temp_df.correctly_identified==True].shape[0] / temp_df.shape[0]\n",
    "    neg_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==0)].shape[0]\n",
    "    positive_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==1)].shape[0]\n",
    "    neg_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==0)].shape[0] / neg_samples\n",
    "    pos_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==1)].shape[0] / positive_samples\n",
    "    print(\"Total {} Drugs in Test Samples:  {} --- Model Accuracy {}\".format(name, temp_df.shape[0], accuracy))\n",
    "    print(\"\\t\\t{} Drugs in Positive Interactions: {}    Accuracy:{}\".format(name, positive_samples, pos_acc))\n",
    "    print(\"\\t\\t{} Drugs in Negative Interactions: {}    Accuracy:{}\".format(name, neg_samples, neg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Accuracy for pairs with min_degree < N\n",
    "N = distribution_df.max_degree.max()\n",
    "print(\"Node2Vec Experiment Min Degree: {}\".format(N))\n",
    "mol2vec_min_degree_recall_list = []\n",
    "mol2vec_min_degree_accuracy_list = []\n",
    "for n in tqdm(range(1,N+1)):\n",
    "    lddf = distribution_df[distribution_df.min_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    mol2vec_min_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    mol2vec_min_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "N = distribution_df.max_degree.max()\n",
    "print(\"Node2Vec Experiment Mean Degree: {}\".format(N))\n",
    "mol2vec_mean_degree_recall_list = []\n",
    "mol2vec_mean_degree_accuracy_list = []\n",
    "for n in tqdm(range(6,N+1)):\n",
    "    lddf = distribution_df[distribution_df.mean_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    mol2vec_mean_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    mol2vec_mean_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "N = distribution_df.max_degree.max()\n",
    "print(\"Node2Vec Experiment Max Degree: {}\".format(N))\n",
    "mol2vec_max_degree_recall_list = []\n",
    "mol2vec_max_degree_accuracy_list = []\n",
    "for n in tqdm(range(11,N+1)):\n",
    "    lddf = distribution_df[distribution_df.max_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    mol2vec_max_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    mol2vec_max_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "C = distribution_df.core_diff.max()\n",
    "mol2vec_core_diff_recall_list = []\n",
    "mol2vec_core_diff_accuracy_list = []\n",
    "print(\"Node2Vec Experiment Max Core Difference: {}\".format(C))\n",
    "for c in tqdm(range(0,C+1)):\n",
    "    lddf = distribution_df[distribution_df.core_diff <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    mol2vec_core_diff_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    mol2vec_core_diff_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "B = distribution_df.b_centrality.max()\n",
    "mol2vec_b_centrality_recall_list = []\n",
    "mol2vec_b_centrality_accuracy_list = []\n",
    "print(\"Node2Vec Experiment b_centrality: {}\".format(B))\n",
    "for c in tqdm(np.arange(0.01,B+0.1, 0.1)):\n",
    "    lddf = distribution_df[distribution_df.b_centrality <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    mol2vec_b_centrality_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    mol2vec_b_centrality_accuracy_list.append(100.0*correct/(correct + false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save distribution_df for mol2vec\n",
    "m2v_distribution_df = distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### GRAPH EMBEDDING EXPERIMENT ##########################\n",
    "import networkx as nx\n",
    "#cid_list = list(set(phsu_interacts_df.snode_cid.unique()).union(phsu_interacts_df.onode_cid.unique()))\n",
    "nodes_df = valid_interactions.SDID.unique()\n",
    "edges_df = valid_interactions[['SDID','ODID']]\n",
    "\n",
    "interactions_graph=nx.from_pandas_edgelist(edges_df, \"SDID\", \"ODID\")\n",
    "print(\"Creating graph for relation: Interactions --- Nodes: {}, Edges: {}\".format(len(interactions_graph.nodes()), len(interactions_graph.edges())))\n",
    "\n",
    "######## Study the Graph\n",
    "\n",
    "#print the conneted components\n",
    "print ('Size of connected components')\n",
    "comp=list(nx.connected_components(interactions_graph))\n",
    "print ('#Connected components', nx.number_connected_components(interactions_graph))\n",
    "\n",
    "#degree centrality\n",
    "degreeNodes=dict(nx.degree(interactions_graph))\n",
    "\n",
    "# core degrees of Nodes\n",
    "temp_graph = nx.Graph.copy(interactions_graph)\n",
    "temp_graph.remove_edges_from(nx.selfloop_edges(temp_graph))\n",
    "core_dict = nx.algorithms.core.core_number(temp_graph)\n",
    "\n",
    "# edge betweeness centrality --- Loaded from pickle\n",
    "# edge_betweeness_centrality = nx.algorithms.centrality.edge_betweenness_centrality(interactions_graph)\n",
    "\n",
    "# a histogram\n",
    "hg=nx.degree_histogram(interactions_graph)\n",
    "plt.plot(sorted(hg,reverse=True),'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Number of nodes')\n",
    "plt.show()\n",
    "\n",
    "numberOfNodes = np.sum(hg)\n",
    "hg=nx.degree_histogram(interactions_graph)\n",
    "plt.plot(sorted(hg,reverse=True)/numberOfNodes,'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Percentage of Nodes')\n",
    "plt.show()\n",
    "\n",
    "hg=nx.degree_histogram(interactions_graph)\n",
    "plt.plot((np.arange(1,len(hg)+1)),sorted(np.log10(hg),reverse=True),'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Number of nodes (log scale)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(interactions_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomly remove edges\n",
    "from random import sample\n",
    "from math import floor\n",
    "REMOVE_FACTOR = 0.99\n",
    "print(\"All edges: {}\".format(len(interactions_graph.edges())))\n",
    "to_remove=sample(interactions_graph.edges(),k=floor(REMOVE_FACTOR*len(interactions_graph.edges()))) #removing of edges\n",
    "interactions_graph_filtered = interactions_graph.copy()\n",
    "interactions_graph_filtered.remove_edges_from(to_remove)\n",
    "print(\"Total edges after removing random samples: {}\".format(len(interactions_graph_filtered.edges())))\n",
    "# print(G.edges())\n",
    "removed_set = set(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Study the Filtered Graph\n",
    "\n",
    "#print the conneted components\n",
    "print ('Size of connected components')\n",
    "comp=list(nx.connected_components(interactions_graph_filtered))\n",
    "print ('#Connected components', nx.number_connected_components(interactions_graph_filtered))\n",
    "\n",
    "#degree centrality\n",
    "degreeNodesFiltered=dict(nx.degree(interactions_graph_filtered))\n",
    "\n",
    "# core degrees of Nodes\n",
    "temp_graph = nx.Graph.copy(interactions_graph_filtered)\n",
    "temp_graph.remove_edges_from(nx.selfloop_edges(temp_graph))\n",
    "core_dictFiltered = nx.algorithms.core.core_number(temp_graph)\n",
    "\n",
    "# a histogram\n",
    "hg=nx.degree_histogram(interactions_graph_filtered)\n",
    "plt.plot(sorted(hg,reverse=True),'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Number of nodes')\n",
    "plt.show()\n",
    "\n",
    "numberOfNodes = np.sum(hg)\n",
    "hg=nx.degree_histogram(interactions_graph_filtered)\n",
    "plt.plot(sorted(hg,reverse=True)/numberOfNodes,'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Percentage of Nodes')\n",
    "plt.show()\n",
    "\n",
    "hg=nx.degree_histogram(interactions_graph_filtered)\n",
    "plt.plot((np.arange(1,len(hg)+1)),sorted(np.log10(hg),reverse=True),'.')\n",
    "plt.xlabel('node degrees')\n",
    "plt.ylabel('Number of nodes (log scale)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(interactions_graph_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from node2vec import Node2Vec\n",
    "from nodevectors import Node2Vec\n",
    "# Generate walks with Original Node2Vec\n",
    "# node2vec = Node2Vec(interactions_graph_filtered, dimensions=128, walk_length=16, num_walks=100, workers = 1)\n",
    "# model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Generate walks with Graph2Vec (nodevectors lib)\n",
    "# Fit embedding model to graph\n",
    "g2v = Node2Vec(n_components=128, walklen=16, epochs=100)\n",
    "# way faster than other node2vec implementations\n",
    "# Graph edge weights are handled automatically\n",
    "g2v.fit(interactions_graph_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = g2v.model.wv.index2word  # list of node IDs\n",
    "# perform conversion of ids to ints\n",
    "node_ids = list(node_ids)\n",
    "embedding_vectors = g2v.model.wv.vectors\n",
    "n2v_df = pd.DataFrame(list(zip(node_ids, embedding_vectors)),columns=['DATABASE_ID','node2vec'])\n",
    "# using dictionary comprehension\n",
    "# to convert lists to dictionary\n",
    "cid2node_vec_dict = {node_ids[i]: embedding_vectors[i] for i in range(len(node_ids))}\n",
    "### uddate substances df with node2vec vector\n",
    "structures_df[\"node2vec\"] = structures_df[\"DATABASE_ID\"].map(cid2node_vec_dict)\n",
    "########### RUN UP TO THIS CELL FOR HYBRID EXPERIMENT ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import random\n",
    "#### Create Classification Dataframe with true samples {features_vector, 1} ####\n",
    "classification_train_rows_list = []\n",
    "classification_test_rows_list = []\n",
    "MOVE_TO_TRAIN_PROB = 0.50\n",
    "\n",
    "for index, row in tqdm(valid_interactions.iterrows(), total=valid_interactions.shape[0]):\n",
    "    s_node2vec = cid2node_vec_dict[row['SDID']]\n",
    "    scid = row['SDID']\n",
    "    o_node2vec = cid2node_vec_dict[row['ODID']]\n",
    "    ocid = row['ODID']\n",
    "    is_test = (row['SDID'],row['ODID']) in removed_set or (row['ODID'], row['SDID']) in removed_set\n",
    "    try:   \n",
    "        fv_direct = np.concatenate((s_node2vec.tolist(), o_node2vec.tolist()))\n",
    "        if is_test and (random() > MOVE_TO_TRAIN_PROB): # with prob to move to train\n",
    "            # fv_opposite = np.concatenate((o_node2vec, s_node2vec)) ### track opposite direction of interacts relationship\n",
    "            classification_test_rows_list.append([fv_direct, 1, scid, ocid])\n",
    "            # classification_test_rows_list.append([fv_opposite, 1])  ### opposite is not needed since neo4j contains reverse of each edge\n",
    "        else:\n",
    "            classification_train_rows_list.append([fv_direct, 1, scid, ocid])\n",
    "    except IndexError:\n",
    "        print(\"IndexError on index: {} for subject: {} and object: {}\".format(index, s_node2vec, o_node2vec))\n",
    "\n",
    "    \n",
    "classification_train_df = pd.DataFrame(data = classification_train_rows_list, columns=['features_vector', 'class', 'SDID', 'ODID']) \n",
    "classification_test_df = pd.DataFrame(data = classification_test_rows_list, columns=['features_vector', 'class', 'SDID', 'ODID']) \n",
    "# Remove duplicates\n",
    "#classification_df = classification_df[~classification_df['features_vector'].apply(tuple).duplicated()]\n",
    "print(\"Classification Train DF shape:{}\".format(classification_train_df.shape))\n",
    "print(\"Classification Test DF shape:{}\".format(classification_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, choice\n",
    "# Add samples with substances that do not interact\n",
    "# Generate random tuples from the substances list, and if they do not appear on the interactions_df add them as a non interaction\n",
    "\n",
    "def getSetWithAllPairs(df, col1, col2):\n",
    "    return set(list(zip(df[col1],df[col2])))\n",
    " \n",
    "def getNegSamples(pairs_to_avoid, substances_df, neg_sample_size, result_list, result_idx, bad_cids):\n",
    "    subs_cid_list = list(set().union(valid_interactions['SDID'].unique(), valid_interactions['ODID'].unique()))\n",
    "    neg_examples = []\n",
    "    while len(neg_examples) < neg_sample_size:\n",
    "        # Generate random pair\n",
    "        cid_a = choice(subs_cid_list)\n",
    "        cid_b = choice(subs_cid_list)\n",
    "        if((cid_a in bad_cids) or (cid_b in bad_cids)):\n",
    "            continue\n",
    "        # flag1 = not phsu_interacts_df.loc[(phsu_interacts_df['snode_cid'] == cid_a) & (phsu_interacts_df['onode_cid'] == cid_b)].any().all()\n",
    "        # flag2 = not phsu_interacts_df.loc[(phsu_interacts_df['snode_cid'] == cid_b) & (phsu_interacts_df['onode_cid'] == cid_a)].any().all()\n",
    "        flag1 = (cid_a, cid_b) not in pairs_to_avoid\n",
    "        flag2 = (cid_b, cid_a) not in pairs_to_avoid\n",
    "        if(cid_a != cid_b and flag1 and flag2):\n",
    "            s_node2vec = cid2node_vec_dict[cid_a]\n",
    "            o_node2vec = cid2node_vec_dict[cid_b]\n",
    "            try:   \n",
    "                # fv_direct = np.concatenate((s_node2vec.tolist(), o_node2vec.tolist()))\n",
    "                # fv_opposite = np.concatenate((o_node2vec.tolist(), s_node2vec.tolist())) ### track opposite direction of interacts relationship\n",
    "                neg_examples.append([0, cid_a, cid_b])\n",
    "                neg_examples.append([0, cid_b, cid_a])\n",
    "            except IndexError:\n",
    "                print(\"IndexError on index: {} for subject: {} and object: {}\".format(index, s_node2vec, o_node2vec))\n",
    "    result_list[result_idx-1] = neg_examples\n",
    "    return\n",
    "\n",
    "thread_list = []\n",
    "NEG_SAMPLE_SIZE = 700000 # For each thread\n",
    "THREADS = 4\n",
    "result_list = [None]*THREADS\n",
    "pairs_2_avoid = getSetWithAllPairs(valid_interactions, 'SDID', 'ODID')\n",
    "bad_cids = {2122, 6227}\n",
    "print(\"Initiating Multithreading Execution\")\n",
    "for i in range(THREADS):\n",
    "    thread = Thread(target = getNegSamples, args = [pairs_2_avoid,valid_interactions, NEG_SAMPLE_SIZE, result_list, i, bad_cids])\n",
    "    thread_list.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for t in thread_list:\n",
    "    t.join()\n",
    "\n",
    "print(\"Exited Multithreaded Section\")\n",
    "\n",
    "neg_examples = []\n",
    "for l in result_list:\n",
    "    if l != None: neg_examples.extend(l)\n",
    "            \n",
    "print(\"Creating Dataframe with negative samples\")\n",
    "neg_examples_df = pd.DataFrame(data = neg_examples, columns=['class', 'SDID', 'ODID'])\n",
    "print(\"Removing Duplicate vectors\") \n",
    "#Remove duplicates\n",
    "# neg_examples_df = neg_examples_df[~neg_examples_df['features_vector'].apply(str).duplicated()] \n",
    "neg_examples_df = neg_examples_df.drop_duplicates()\n",
    "\n",
    "# Create features_vector column\n",
    "neg_examples_df[\"features_vector\"] = neg_examples_df.apply(lambda r: np.concatenate((cid2node_vec_dict[r.SDID].tolist(), cid2node_vec_dict[r.ODID].tolist())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "### Split negative examples df to train and test\n",
    "from math import floor\n",
    "test_sample_factor = 0.33\n",
    "neg_examples_test = neg_examples_df.sample(floor(test_sample_factor * neg_examples_df.shape[0]))\n",
    "neg_examples_train = neg_examples_df.drop(neg_examples_test.index)\n",
    "\n",
    "### Create train and test dataframes\n",
    "train_df = pd.concat([classification_train_df, neg_examples_train], axis = 0).sample(frac = 1)\n",
    "test_df = pd.concat([classification_test_df, neg_examples_test], axis = 0).sample(frac = 1)\n",
    "\n",
    "X_train, y_train, train_pairs = train_df[['features_vector']], train_df[['class']], train_df[['SDID', 'ODID']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=69)\n",
    "X_test, y_test, test_pairs = test_df[['features_vector']], test_df[['class']], test_df[['SDID', 'ODID']]\n",
    "### Create Test Dataframe with positive and negative examples of interactions\n",
    "\n",
    "print(\"Shape of X_train: {} ------ Class_0: {} -- Class_1: {}\".format(X_train.shape[0],  y_train[(y_train.values==0)].count(), y_train[(y_train.values==1)].count()))\n",
    "print(\"Shape of X_test: {}  ------ Class_0: {} -- Class_1: {}\".format(X_test.shape[0], y_test[(y_test.values==0)].count(), y_test[(y_test.values==1)].count()))\n",
    "print(\"Shape of X_val: {}  ------ Class_0: {}  -- Class_1: {}\".format(X_val.shape[0], y_val[(y_val.values==0)].count(), y_val[(y_val.values==1)].count()))\n",
    "\n",
    "print(\"Plotting counts for Test Data\")\n",
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_test, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_train, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_val, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataclass TrainData(Dataset):\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train.features_vector.values.tolist()), \n",
    "                       torch.FloatTensor(list(map(lambda x: x[0], y_train.values.tolist()))))\n",
    "## test data    \n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test.features_vector.values.tolist()))\n",
    "val_data = TestData(torch.FloatTensor(X_val.features_vector.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()        # Number of input features is 256.\n",
    "        self.layer_1 = nn.Linear(256, 512)\n",
    "        self.layer_2 = nn.Linear(512, 256)\n",
    "        self.layer_3 = nn.Linear(256, 128)\n",
    "        self.layer_out = nn.Linear(128, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Define Model\n",
    "model = BinaryClassification()\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Train the model\n",
    "# Model Parameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING = 3\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bad_progress_counter = 0\n",
    "model_epoch = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=1)\n",
    "\n",
    "print(device)\n",
    "model.train()\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "        y_pred = model(X_batch.cuda()).to(device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    # validation\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val_pred = model(X_batch)\n",
    "            y_val_pred = torch.sigmoid(y_val_pred)\n",
    "            y_pred_tag = torch.round(y_val_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    val_acc = accuracy_score(y_pred_list, y_val)\n",
    "    if e==1: \n",
    "        best_val_acc = val_acc\n",
    "    elif best_val_acc <= val_acc:\n",
    "        bad_progress_counter = 0\n",
    "    else:\n",
    "        bad_progress_counter +=1\n",
    "    \n",
    "    if bad_progress_counter == EARLY_STOPPING:\n",
    "        print(\"Applying Early Stop\")\n",
    "        break\n",
    "    # End of validation step\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Val Acc: {val_acc}')\n",
    "    if(val_acc > best_val_acc):\n",
    "        node2vec = model   #Store model\n",
    "        model_epoch = e\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "print(\"Keeping model from epoch: {}\".format(model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model\n",
    "y_pred_list = []\n",
    "node2vec.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in tqdm(test_loader, total = len(test_loader)):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = node2vec(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, y_pred_list)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE Result Data\n",
    "# with open('drugbank_experiment_pickles/node2vec_large_full_results.pickle', 'wb') as handle:\n",
    "#     pickle.dump((test_pairs, y_test, y_pred_list), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### LOAD Data from Pickles --- folder: drugbank_experiment_pickless\n",
    "with open('drugbank_experiment_pickles/node2vec_large_full_results.pickle', 'rb') as handle:\n",
    "    test_pairs_n, y_test_n, y_pred_list_n = pickle.load(handle)\n",
    "test_pairs, y_test, y_pred_list = test_pairs_n, y_test_n, y_pred_list_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "###### Compute Useful Test Results regadring Graph Properites on the Test Set's Nodes\n",
    "# test_pairs, degreeNodes, y_test, y_pred\n",
    "correct_degrees, min_correct_degrees, max_correct_degrees = [], [], []\n",
    "false_degrees, min_false_degrees, max_false_degrees = [], [], []\n",
    "correct_core_diffs, false_core_diffs = [], []\n",
    "correct_b_centrality, false_b_centrality = [], []\n",
    "pairs = test_pairs.values\n",
    "test_values = y_test.values\n",
    "for i in range(len(pairs)):\n",
    "    scid, ocid = pairs[i][0], pairs[i][1]\n",
    "    sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "    score, ocore = core_dict[scid], core_dict[ocid]\n",
    "    if int(test_values[i][0]) == int(y_pred_list[i]):   # Correct decision for NN\n",
    "        correct_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_correct_degrees.append(min(sdegree, odegree))\n",
    "        max_correct_degrees.append(max(sdegree, odegree))\n",
    "        correct_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            correct_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        false_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_false_degrees.append(min(sdegree, odegree))\n",
    "        max_false_degrees.append(max(sdegree, odegree))\n",
    "        false_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            false_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(\"Mean Correct Degrees: {}\".format(mean(correct_degrees)))\n",
    "print(\"Mean MIN Correct Degrees: {}\".format(mean(min_correct_degrees)))\n",
    "print(\"Mean MAX Correct Degrees: {}\".format(mean(max_correct_degrees)))\n",
    "print(\"Mean Correct Core Differences: {}\".format(mean(correct_core_diffs)))\n",
    "print(\"Mean Correct Betweenus Centrality: {}\".format(mean(correct_b_centrality)))\n",
    "print(\"Mean False Degrees: {}\".format(mean(false_degrees)))\n",
    "print(\"Mean MIN False Degrees: {}\".format(mean(min_false_degrees)))\n",
    "print(\"Mean MAX False Degrees: {}\".format(mean(max_false_degrees)))\n",
    "print(\"Mean False Core Differences: {}\".format(mean(false_core_diffs)))\n",
    "print(\"Mean False Betweenus Centrality: {}\".format(mean(false_b_centrality)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for Plots\n",
    "resolution = 1000\n",
    "b_centrality_limit = 3\n",
    "distribution_dict = {}\n",
    "for i in tqdm(range(len(pairs))):\n",
    "   scid, ocid = pairs[i][0], pairs[i][1]\n",
    "   sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "   score, ocore = core_dict[scid], core_dict[ocid]\n",
    "   is_positive =  int(test_values[i])\n",
    "   correctly_identified = int(test_values[i]) == int(y_pred_list[i])\n",
    "   mean_degree = 1.0*(sdegree + odegree)/2\n",
    "   min_degree = min(sdegree, odegree)\n",
    "   max_degree = max(sdegree, odegree)\n",
    "   core_diff = abs(score-ocore)\n",
    "   try:\n",
    "      b_centrallity = 10**6*edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)]\n",
    "   except: \n",
    "      b_centrallity = None\n",
    "   distribution_dict[(scid, ocid)] = [is_positive, correctly_identified, mean_degree, min_degree, max_degree, core_diff, b_centrallity]\n",
    "\n",
    "distribution_df = pd.DataFrame.from_dict(data = distribution_dict, orient = 'index' , columns= ['is_positive', 'correctly_identified', 'mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters for results plots file save\n",
    "resultsPath = \"result_plots/large_drugbank_results/graph2vec/train_with_01_percent/\"   ### CHANGE THIS FOR EVERY EXPERIMENT\n",
    "metrics_dict = {\"mean_degree\":\"Mean Degree\", \"min_degree\":\"Min Degree\", \"max_degree\":\"Max Degree\", \"core_diff\":\"Core Difference\", \"b_centrality\":\"Betweenness Centrality\"}\n",
    "\n",
    "### Plot Group A: Histogram grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=False, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Frequency Histogram of Classifier's predictions\", ylabel='Frequency', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupA_Frequency Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group AX: Density grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    # Normalize\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Density Histogram of Classifier's predictions\", ylabel='Density', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupAX_Density Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "   \n",
    "### Plot Group B: Density grouped by categories in separate subplots\n",
    "\n",
    "param_list = []\n",
    "for pos in [0,1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "colors = ['tab:green', 'tab:blue', 'tab:red', 'tab:cyan']\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "param_list = []\n",
    "for pos in [1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['b_centrality']:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        x = x[x.between(0,b_centrality_limit)]\n",
    "\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group C: KDE \n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    dv_list = []\n",
    "    i=0\n",
    "    for pos in [0,1]:\n",
    "        for corr in [True, False]:\n",
    "            dv = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)].value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs, bw_method=0.03)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "for selector in ['b_centrality']:\n",
    "    dv_list = []\n",
    "    i=2\n",
    "    for pos in [1]:\n",
    "        for corr in [True, False]:\n",
    "            x = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)]\n",
    "            x = x[x.between(0,b_centrality_limit)]\n",
    "            dv = x.value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "### Compute Stats\n",
    "print(\"Total Negative Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Negative Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "print(\"Total Positive Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Positive Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "\n",
    "#### B-centrality-stats\n",
    "print(\"\\nB centrality plots where limited on samples with b_centrality < {}\".format(b_centrality_limit))\n",
    "print(\"Total nodes with b_centrality <  {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality < b_centrality_limit)].shape[0]))\n",
    "print(\"Total nodes with b_centrality >= {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality >= b_centrality_limit)].shape[0]))\n",
    "\n",
    "\n",
    "print(\"\\nKL Divergence Results\")\n",
    "### KL Divergence Computation\n",
    "print(\"KL Divergence between distributions from Classifier choices\")\n",
    "kl_list = []\n",
    "for index, selector in enumerate(['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']):\n",
    "    print(\"---\" + selector + \" ~ KL(Correct | False): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==True][selector], distribution_df[distribution_df.correctly_identified==False][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(False | Correct): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==False][selector], distribution_df[distribution_df.correctly_identified==True][selector])))\n",
    "\n",
    "####### Evaluate Model's Predictions on Drug Groups\n",
    "distribution_df['sgroup'] = distribution_df.index.map(lambda x: groups_dict[x[0]])\n",
    "distribution_df['ogroup'] =distribution_df.index.map(lambda x: groups_dict[x[1]])\n",
    "\n",
    "# Print (ordered) Accuracy and total samples for each category (when at least one drug on the interaction belongs to a category)\n",
    "group_names = ['approved', 'investigational', 'experimental', 'illicit', 'vet_approved', 'withdrawn', 'nutraceutical']\n",
    "\n",
    "for name in group_names:\n",
    "    temp_df = distribution_df[(distribution_df.sgroup==name) | (distribution_df.ogroup==name)]\n",
    "    accuracy = temp_df[temp_df.correctly_identified==True].shape[0] / temp_df.shape[0]\n",
    "    neg_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==0)].shape[0]\n",
    "    positive_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==1)].shape[0]\n",
    "    neg_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==0)].shape[0] / neg_samples\n",
    "    pos_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==1)].shape[0] / positive_samples\n",
    "    print(\"Total {} Drugs in Test Samples:  {} --- Model Accuracy {}\".format(name, temp_df.shape[0], accuracy))\n",
    "    print(\"\\t\\t{} Drugs in Positive Interactions: {}    Accuracy:{}\".format(name, positive_samples, pos_acc))\n",
    "    print(\"\\t\\t{} Drugs in Negative Interactions: {}    Accuracy:{}\".format(name, neg_samples, neg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Accuracy for pairs with min_degree < N\n",
    "N = distribution_df.max_degree.max()\n",
    "print(\"Node2Vec Experiment Max Degree: {}\".format(N))\n",
    "node2vec_min_degree_recall_list = []\n",
    "node2vec_min_degree_accuracy_list = []\n",
    "for n in tqdm(range(1,N+1)):\n",
    "    lddf = distribution_df[distribution_df.min_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    node2vec_min_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    node2vec_min_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "node2vec_mean_degree_recall_list = []\n",
    "node2vec_mean_degree_accuracy_list = []\n",
    "for n in tqdm(range(6,N+1)):\n",
    "    lddf = distribution_df[distribution_df.mean_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    node2vec_mean_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    node2vec_mean_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "node2vec_max_degree_recall_list = []\n",
    "node2vec_max_degree_accuracy_list = []\n",
    "for n in tqdm(range(11,N+1)):\n",
    "    lddf = distribution_df[distribution_df.max_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    node2vec_max_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    node2vec_max_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "C = distribution_df.core_diff.max()\n",
    "node2vec_core_diff_recall_list = []\n",
    "node2vec_core_diff_accuracy_list = []\n",
    "print(\"Node2Vec Experiment Max Core Difference: {}\".format(C))\n",
    "for c in tqdm(range(0,C+1)):\n",
    "    lddf = distribution_df[distribution_df.core_diff <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    node2vec_core_diff_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    node2vec_core_diff_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "B = distribution_df.b_centrality.max()\n",
    "node2vec_b_centrality_recall_list = []\n",
    "node2vec_b_centrality_accuracy_list = []\n",
    "print(\"Node2Vec Experiment B Centrality Max: {}\".format(B))\n",
    "for c in tqdm(np.arange(0.01,B+0.1, 0.1)):\n",
    "    lddf = distribution_df[distribution_df.b_centrality <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    node2vec_b_centrality_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    node2vec_b_centrality_accuracy_list.append(100.0*correct/(correct + false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save node2vec distribution_df\n",
    "n2v_distribution_df = distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cross-Experiment KL Divergence\n",
    "print(\"\\nCross-Experiment KL Divergence Results\")\n",
    "### KL Divergence Computation\n",
    "print(\"KL Divergence between distributions from Classifier choices\")\n",
    "kl_list = []\n",
    "for index, selector in enumerate(['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']):\n",
    "    print(\"---\" + selector + \" ~ KL(N2V Correct | M2V Corrent): {}\"\\\n",
    "        .format(KL(n2v_distribution_df[n2v_distribution_df.correctly_identified==True][selector], m2v_distribution_df[m2v_distribution_df.correctly_identified==True][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(M2V Correct | N2V Corrent): {}\"\\\n",
    "        .format(KL(m2v_distribution_df[m2v_distribution_df.correctly_identified==True][selector], n2v_distribution_df[n2v_distribution_df.correctly_identified==True][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(N2V False | M2V False): {}\"\\\n",
    "        .format(KL(n2v_distribution_df[n2v_distribution_df.correctly_identified==False][selector], m2v_distribution_df[m2v_distribution_df.correctly_identified==False][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(M2V False | N2V False): {}\\n\"\\\n",
    "        .format(KL(m2v_distribution_df[m2v_distribution_df.correctly_identified==False][selector], n2v_distribution_df[n2v_distribution_df.correctly_identified==False][selector])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Hybrid Model Experiment ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_df = structures_df.dropna(subset = ['node2vec', 'mol2vec'])\n",
    "structures_df['mixedvec'] = structures_df.apply(lambda x: np.concatenate((x.node2vec, x.mol2vec.vec)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import random\n",
    "#### Create Classification Dataframe with true samples {features_vector, 1} ####\n",
    "classification_train_rows_list = []\n",
    "classification_test_rows_list = []\n",
    "MOVE_TO_TRAIN_PROB = 0.70\n",
    "mv_dict = dict(zip(structures_df.DATABASE_ID, structures_df.mixedvec))\n",
    "\n",
    "for index, row in tqdm(valid_interactions.iterrows(), total=valid_interactions.shape[0]):\n",
    "    s_mv = mv_dict[row['SDID']]\n",
    "    scid = row['SDID']\n",
    "    o_mv = mv_dict[row['ODID']]\n",
    "    ocid = row['ODID']\n",
    "    is_test = (row['SDID'],row['ODID']) in removed_set or (row['ODID'], row['SDID']) in removed_set\n",
    "    try:   \n",
    "        fv_direct = np.concatenate((s_mv, o_mv))\n",
    "        if is_test and (random() > MOVE_TO_TRAIN_PROB): # with prob to move to train\n",
    "            # fv_opposite = np.concatenate((o_node2vec, s_node2vec)) ### track opposite direction of interacts relationship\n",
    "            classification_test_rows_list.append([fv_direct, 1, scid, ocid])\n",
    "            # classification_test_rows_list.append([fv_opposite, 1])  ### opposite is not needed since neo4j contains reverse of each edge\n",
    "        else:\n",
    "            classification_train_rows_list.append([fv_direct, 1, scid, ocid])\n",
    "    except IndexError:\n",
    "        print(\"IndexError on index: {} for subject: {} and object: {}\".format(index, scid, ocid))\n",
    "\n",
    "    \n",
    "classification_train_df = pd.DataFrame(data = classification_train_rows_list, columns=['features_vector', 'class', 'SDID', 'ODID']) \n",
    "classification_test_df = pd.DataFrame(data = classification_test_rows_list, columns=['features_vector', 'class', 'SDID', 'ODID']) \n",
    "# Remove duplicates\n",
    "#classification_df = classification_df[~classification_df['features_vector'].apply(tuple).duplicated()]\n",
    "print(\"Classification Train DF shape:{}\".format(classification_train_df.shape))\n",
    "print(\"Classification Test DF shape:{}\".format(classification_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, choice\n",
    "# Add samples with substances that do not interact\n",
    "# Generate random tuples from the substances list, and if they do not appear on the interactions_df add them as a non interaction\n",
    "\n",
    "def getSetWithAllPairs(df, col1, col2):\n",
    "    return set(list(zip(df[col1],df[col2])))\n",
    " \n",
    "def getNegSamples(pairs_to_avoid, substances_df, neg_sample_size, result_list, result_idx, bad_cids):\n",
    "    subs_cid_list = list(set().union(valid_interactions['SDID'].unique(), valid_interactions['ODID'].unique()))\n",
    "    neg_examples = []\n",
    "    while len(neg_examples) < neg_sample_size:\n",
    "        # Generate random pair\n",
    "        cid_a = choice(subs_cid_list)\n",
    "        cid_b = choice(subs_cid_list)\n",
    "        if((cid_a in bad_cids) or (cid_b in bad_cids)):\n",
    "            continue\n",
    "        # flag1 = not phsu_interacts_df.loc[(phsu_interacts_df['snode_cid'] == cid_a) & (phsu_interacts_df['onode_cid'] == cid_b)].any().all()\n",
    "        # flag2 = not phsu_interacts_df.loc[(phsu_interacts_df['snode_cid'] == cid_b) & (phsu_interacts_df['onode_cid'] == cid_a)].any().all()\n",
    "        flag1 = (cid_a, cid_b) not in pairs_to_avoid\n",
    "        flag2 = (cid_b, cid_a) not in pairs_to_avoid\n",
    "        if(cid_a != cid_b and flag1 and flag2):\n",
    "            #s_mv = structures_df[structures_df.DATABASE_ID == row[cid_a]].mixedvec\n",
    "            #o_mv = structures_df[structures_df.DATABASE_ID == row[cid_b]].mixedvec\n",
    "            try:   \n",
    "                # fv_direct = np.concatenate((s_mv.tolist(), o_mv.tolist()))\n",
    "                # fv_opposite = np.concatenate((o_mv.tolist(), s_mv.tolist())) ### track opposite direction of interacts relationship\n",
    "                neg_examples.append([0, cid_a, cid_b])\n",
    "                neg_examples.append([0, cid_b, cid_a])\n",
    "            except IndexError:\n",
    "                print(\"IndexError on index: {} for subject: {} and object: {}\".format(index, cid_a, cid_b))\n",
    "    result_list[result_idx-1] = neg_examples\n",
    "    return\n",
    "\n",
    "thread_list = []\n",
    "NEG_SAMPLE_SIZE = 700000 # For each thread\n",
    "THREADS = 4\n",
    "result_list = [None]*THREADS\n",
    "pairs_2_avoid = getSetWithAllPairs(valid_interactions, 'SDID', 'ODID')\n",
    "bad_cids = {2122, 6227}\n",
    "print(\"Initiating Multithreading Execution\")\n",
    "for i in range(THREADS):\n",
    "    thread = Thread(target = getNegSamples, args = [pairs_2_avoid,valid_interactions, NEG_SAMPLE_SIZE, result_list, i, bad_cids])\n",
    "    thread_list.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for t in thread_list:\n",
    "    t.join()\n",
    "\n",
    "print(\"Exited Multithreaded Section\")\n",
    "\n",
    "neg_examples = []\n",
    "for l in result_list:\n",
    "    if l != None: neg_examples.extend(l)\n",
    "            \n",
    "print(\"Creating Dataframe with negative samples\")\n",
    "neg_examples_df = pd.DataFrame(data = neg_examples, columns=['class', 'SDID', 'ODID'])\n",
    "print(\"Removing Duplicate vectors\") \n",
    "#Remove duplicates\n",
    "# neg_examples_df = neg_examples_df[~neg_examples_df['features_vector'].apply(str).duplicated()] \n",
    "neg_examples_df = neg_examples_df.drop_duplicates()\n",
    "\n",
    "# Create features_vector column\n",
    "neg_examples_df[\"features_vector\"] = neg_examples_df.apply(lambda r: np.concatenate((mv_dict[r.SDID], mv_dict[r.ODID])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "### Split negative examples df to train and test\n",
    "from math import floor\n",
    "test_sample_factor = 0.33\n",
    "neg_examples_test = neg_examples_df.sample(floor(test_sample_factor * neg_examples_df.shape[0]))\n",
    "neg_examples_train = neg_examples_df.drop(neg_examples_test.index)\n",
    "\n",
    "### Create train and test dataframes\n",
    "train_df = pd.concat([classification_train_df, neg_examples_train], axis = 0).sample(frac = 1)\n",
    "test_df = pd.concat([classification_test_df, neg_examples_test], axis = 0).sample(frac = 1)\n",
    "\n",
    "X_train, y_train, train_pairs = train_df[['features_vector']], train_df[['class']], train_df[['SDID', 'ODID']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=69)\n",
    "X_test, y_test, test_pairs = test_df[['features_vector']], test_df[['class']], test_df[['SDID', 'ODID']]\n",
    "### Create Test Dataframe with positive and negative examples of interactions\n",
    "\n",
    "print(\"Shape of X_train: {} ------ Class_0: {} -- Class_1: {}\".format(X_train.shape[0],  y_train[(y_train.values==0)].count(), y_train[(y_train.values==1)].count()))\n",
    "print(\"Shape of X_test: {}  ------ Class_0: {} -- Class_1: {}\".format(X_test.shape[0], y_test[(y_test.values==0)].count(), y_test[(y_test.values==1)].count()))\n",
    "print(\"Shape of X_val: {}  ------ Class_0: {}  -- Class_1: {}\".format(X_val.shape[0], y_val[(y_val.values==0)].count(), y_val[(y_val.values==1)].count()))\n",
    "\n",
    "print(\"Plotting counts for Test Data\")\n",
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_test, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_train, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'class', data=pd.DataFrame(data=y_val, columns=['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataclass TrainData(Dataset):\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train.features_vector.values.tolist()), \n",
    "                       torch.FloatTensor(list(map(lambda x: x[0], y_train.values.tolist()))))\n",
    "## test data    \n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test.features_vector.values.tolist()))\n",
    "val_data = TestData(torch.FloatTensor(X_val.features_vector.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()        # Number of input features is 256.\n",
    "        self.layer_1 = nn.Linear(856, 2048)\n",
    "        self.layer_2 = nn.Linear(2048, 1024)\n",
    "        self.layer_3 = nn.Linear(1024, 512)\n",
    "        self.layer_out = nn.Linear(512, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(2048)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(1024)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(512)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Define Model\n",
    "model = BinaryClassification()\n",
    "model.cuda()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "### Train the model\n",
    "# Model Parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING = 5\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "bad_progress_counter = 0\n",
    "model_epoch = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=1)\n",
    "\n",
    "print(device)\n",
    "model.train()\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "        y_pred = model(X_batch.cuda()).to(device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    # validation\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val_pred = model(X_batch)\n",
    "            y_val_pred = torch.sigmoid(y_val_pred)\n",
    "            y_pred_tag = torch.round(y_val_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    val_acc = accuracy_score(y_pred_list, y_val)\n",
    "    if e==1: \n",
    "        best_val_acc = val_acc\n",
    "    elif best_val_acc <= val_acc:\n",
    "        bad_progress_counter = 0\n",
    "    else:\n",
    "        bad_progress_counter +=1\n",
    "    \n",
    "    if bad_progress_counter == EARLY_STOPPING:\n",
    "        print(\"Applying Early Stop\")\n",
    "        break\n",
    "    # End of validation step\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Val Acc: {val_acc}')\n",
    "    if(val_acc > best_val_acc):\n",
    "        hybrid = model   #Store model\n",
    "        model_epoch = e\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "print(\"Keeping model from epoch: {}\".format(model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Extra Training\n",
    "print(device)\n",
    "\n",
    "for e in range(51, 80):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "        y_pred = model(X_batch.cuda()).to(device)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1)).to(device)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    # validation\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val_pred = model(X_batch)\n",
    "            y_val_pred = torch.sigmoid(y_val_pred)\n",
    "            y_pred_tag = torch.round(y_val_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    val_acc = accuracy_score(y_pred_list, y_val)\n",
    "    if e==1: \n",
    "        best_val_acc = val_acc\n",
    "    elif best_val_acc <= val_acc:\n",
    "        bad_progress_counter = 0\n",
    "    else:\n",
    "        bad_progress_counter +=1\n",
    "    \n",
    "    if bad_progress_counter == EARLY_STOPPING:\n",
    "        print(\"Applying Early Stop\")\n",
    "        break\n",
    "    # End of validation step\n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} | Val Acc: {val_acc}')\n",
    "    if(val_acc > best_val_acc):\n",
    "        hybrid = model   #Store model\n",
    "        model_epoch = e\n",
    "        best_val_acc = val_acc\n",
    "    \n",
    "print(\"Keeping model from epoch: {}\".format(model_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model\n",
    "y_pred_list = []\n",
    "hybrid.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in tqdm(test_loader, total = len(test_loader)):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = hybrid(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test, y_pred_list)\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE Result Data\n",
    "# with open('drugbank_experiment_pickles/mixedvec_large_full_results.pickle', 'wb') as handle:\n",
    "#     pickle.dump((test_pairs, y_test, y_pred_list), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### LOAD Data from Pickles --- folder: drugbank_experiment_pickless\n",
    "with open('drugbank_experiment_pickles/mixedvec_large_full_results.pickle', 'rb') as handle:\n",
    "    test_pairs_mixed, y_test_mixed, y_pred_list_mixed = pickle.load(handle)\n",
    "test_pairs, y_test, y_pred_list = test_pairs_mixed, y_test_mixed, y_pred_list_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "###### Compute Useful Test Results regadring Graph Properites on the Test Set's Nodes\n",
    "# test_pairs, degreeNodes, y_test, y_pred\n",
    "correct_degrees, min_correct_degrees, max_correct_degrees = [], [], []\n",
    "false_degrees, min_false_degrees, max_false_degrees = [], [], []\n",
    "correct_core_diffs, false_core_diffs = [], []\n",
    "correct_b_centrality, false_b_centrality = [], []\n",
    "pairs = test_pairs.values\n",
    "test_values = y_test.values\n",
    "for i in range(len(pairs)):\n",
    "    scid, ocid = pairs[i][0], pairs[i][1]\n",
    "    sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "    score, ocore = core_dict[scid], core_dict[ocid]\n",
    "    if int(test_values[i][0]) == int(y_pred_list[i]):   # Correct decision for NN\n",
    "        correct_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_correct_degrees.append(min(sdegree, odegree))\n",
    "        max_correct_degrees.append(max(sdegree, odegree))\n",
    "        correct_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            correct_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        false_degrees.append(1.0*(sdegree + odegree)/2)\n",
    "        min_false_degrees.append(min(sdegree, odegree))\n",
    "        max_false_degrees.append(max(sdegree, odegree))\n",
    "        false_core_diffs.append(abs(score-ocore))\n",
    "        try: # will only work if edge really exists \n",
    "            false_b_centrality.append(edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(\"Mean Correct Degrees: {}\".format(mean(correct_degrees)))\n",
    "print(\"Mean MIN Correct Degrees: {}\".format(mean(min_correct_degrees)))\n",
    "print(\"Mean MAX Correct Degrees: {}\".format(mean(max_correct_degrees)))\n",
    "print(\"Mean Correct Core Differences: {}\".format(mean(correct_core_diffs)))\n",
    "print(\"Mean Correct Betweenus Centrality: {}\".format(mean(correct_b_centrality)))\n",
    "print(\"Mean False Degrees: {}\".format(mean(false_degrees)))\n",
    "print(\"Mean MIN False Degrees: {}\".format(mean(min_false_degrees)))\n",
    "print(\"Mean MAX False Degrees: {}\".format(mean(max_false_degrees)))\n",
    "print(\"Mean False Core Differences: {}\".format(mean(false_core_diffs)))\n",
    "print(\"Mean False Betweenus Centrality: {}\".format(mean(false_b_centrality)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for Plots\n",
    "resolution = 1000\n",
    "b_centrality_limit = 3\n",
    "distribution_dict = {}\n",
    "for i in tqdm(range(len(pairs))):\n",
    "   scid, ocid = pairs[i][0], pairs[i][1]\n",
    "   sdegree, odegree = degreeNodes[scid], degreeNodes[ocid]\n",
    "   score, ocore = core_dict[scid], core_dict[ocid]\n",
    "   is_positive =  int(test_values[i])\n",
    "   correctly_identified = int(test_values[i]) == int(y_pred_list[i])\n",
    "   mean_degree = 1.0*(sdegree + odegree)/2\n",
    "   min_degree = min(sdegree, odegree)\n",
    "   max_degree = max(sdegree, odegree)\n",
    "   core_diff = abs(score-ocore)\n",
    "   try:\n",
    "      b_centrallity = 10**6*edge_betweeness_centrality[(scid, ocid)] if ((scid, ocid) in edge_betweeness_centrality.keys()) else edge_betweeness_centrality[(ocid, scid)]\n",
    "   except: \n",
    "      b_centrallity = None\n",
    "   distribution_dict[(scid, ocid)] = [is_positive, correctly_identified, mean_degree, min_degree, max_degree, core_diff, b_centrallity]\n",
    "\n",
    "distribution_df = pd.DataFrame.from_dict(data = distribution_dict, orient = 'index' , columns= ['is_positive', 'correctly_identified', 'mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality'])\n",
    "\n",
    "### Save node2vec distribution_df\n",
    "hybrid_distribution_df = distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters for results plots file save\n",
    "resultsPath = \"result_plots/large_drugbank_results/hybrid/\"   ### CHANGE THIS FOR EVERY EXPERIMENT\n",
    "metrics_dict = {\"mean_degree\":\"Mean Degree\", \"min_degree\":\"Min Degree\", \"max_degree\":\"Max Degree\", \"core_diff\":\"Core Difference\", \"b_centrality\":\"Betweenness Centrality\"}\n",
    "\n",
    "### Plot Group A: Histogram grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=False, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Frequency Histogram of Classifier's predictions\", ylabel='Frequency', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupA_Frequency Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group AX: Density grouped by categories in same plot\n",
    "for x_axis_variable in ['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']:\n",
    "\n",
    "    x1 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)]\n",
    "    x2 = distribution_df[x_axis_variable][(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)]\n",
    "    x3 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)]\n",
    "    x4 = distribution_df[x_axis_variable][(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)]\n",
    "\n",
    "    # Normalize\n",
    "    kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\n",
    "    if x_axis_variable != \"b_centrality\":\n",
    "        plt.hist(x1, **kwargs, color='g', label='Negative Correctly Identified')\n",
    "        plt.hist(x2, **kwargs, color='b', label='Negative Incorrectly Identified')\n",
    "    else:\n",
    "        x3 = x3[x3.between(0,b_centrality_limit)]\n",
    "        x4 = x4[x4.between(0,b_centrality_limit)]\n",
    "    plt.hist(x3, **kwargs, color='r', label='Positive Correctly Identified')\n",
    "    plt.hist(x4, **kwargs, color='c', label='Positive Incorrectly Identified')\n",
    "    plt.gca().set(title=\"Density Histogram of Classifier's predictions\", ylabel='Density', xlabel = metrics_dict[x_axis_variable])\n",
    "    # plt.xlim(50,75)\n",
    "    plt.legend()\n",
    "    imageName = \"groupAX_Density Histogram of Classifier's predictions_\" + x_axis_variable\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "   \n",
    "### Plot Group B: Density grouped by categories in separate subplots\n",
    "\n",
    "param_list = []\n",
    "for pos in [0,1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "colors = ['tab:green', 'tab:blue', 'tab:red', 'tab:cyan']\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "param_list = []\n",
    "for pos in [1]:\n",
    "    for corr in [True, False]:\n",
    "        param_list.append((distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr))\n",
    "titles = ['Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['b_centrality']:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,2.5), dpi=resolution, sharex=True, sharey=True)\n",
    "    for i, (ax, param) in enumerate(zip(axes.flatten(), param_list)):\n",
    "        x = distribution_df[selector][param]\n",
    "        x = x[x.between(0,b_centrality_limit)]\n",
    "\n",
    "        ax.hist(x, alpha=0.5, bins=100, density=True, stacked=True, label=str(titles[i]), color=colors[i])\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_ylabel = \"Density\"\n",
    "        ax.set_xlabel = selector\n",
    "    plt.suptitle('Classifier Predictions: Density of {}'.format(metrics_dict[selector]), y=1)\n",
    "    plt.tight_layout()\n",
    "    imageName = \"groupB_Density Histogram of Classifier's predictions_\" + selector\n",
    "    plt.savefig(resultsPath + imageName +'.png', dpi=resolution, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "### Plot Group C: KDE \n",
    "titles = ['Negative Correctly Identified', 'Negative Incorrectly Identified', 'Positive Correctly Identified', 'Positive Incorrectly Identified']\n",
    "\n",
    "for selector in ['mean_degree', 'min_degree', 'max_degree', 'core_diff']:\n",
    "    dv_list = []\n",
    "    i=0\n",
    "    for pos in [0,1]:\n",
    "        for corr in [True, False]:\n",
    "            dv = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)].value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs, bw_method=0.03)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############# Repeat for Distributions that recognize only positive edges\n",
    "for selector in ['b_centrality']:\n",
    "    dv_list = []\n",
    "    i=2\n",
    "    for pos in [1]:\n",
    "        for corr in [True, False]:\n",
    "            x = distribution_df[selector][(distribution_df.is_positive==pos) & (distribution_df.correctly_identified==corr)]\n",
    "            x = x[x.between(0,b_centrality_limit)]\n",
    "            dv = x.value_counts()\n",
    "            #dv = dv/dv.sum()\n",
    "            dv = dv.rename(titles[i]+ \" -  \" + selector)\n",
    "            i += 1\n",
    "            dv = dv.sort_index()\n",
    "            dv_list.append(dv)\n",
    "    dist_df = pd.concat(dv_list, axis=1)\n",
    "    dist_df = pd.concat(dv_list, axis=1).fillna(0)\n",
    "    kwargs = dict(title=\"Kernel Density Estimate for Classifier's Predictions: {}\".format(metrics_dict[selector]), color=colors)\n",
    "    figSrc = dist_df.plot.kde(**kwargs)\n",
    "    fig = figSrc.get_figure()\n",
    "    imageName = \"groupC_KDE of Classifier's predictions_\" + selector\n",
    "    fig.savefig(resultsPath + imageName + \".png\", dpi=resolution, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "### Compute Stats\n",
    "print(\"Total Negative Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Negative Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==0) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "print(\"Total Positive Correctly Identified Samples:     {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==True)].shape[0]))\n",
    "print(\"Total Positive Incorrectly Identified Samples:   {}\".format(distribution_df[(distribution_df.is_positive==1) & (distribution_df.correctly_identified==False)].shape[0]))\n",
    "\n",
    "#### B-centrality-stats\n",
    "print(\"\\nB centrality plots where limited on samples with b_centrality < {}\".format(b_centrality_limit))\n",
    "print(\"Total nodes with b_centrality <  {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality < b_centrality_limit)].shape[0]))\n",
    "print(\"Total nodes with b_centrality >= {}:   {}\".format(b_centrality_limit, distribution_df[(distribution_df.b_centrality >= b_centrality_limit)].shape[0]))\n",
    "\n",
    "\n",
    "print(\"\\nKL Divergence Results\")\n",
    "### KL Divergence Computation\n",
    "print(\"KL Divergence between distributions from Classifier choices\")\n",
    "kl_list = []\n",
    "for index, selector in enumerate(['mean_degree', 'min_degree', 'max_degree', 'core_diff', 'b_centrality']):\n",
    "    print(\"---\" + selector + \" ~ KL(Correct | False): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==True][selector], distribution_df[distribution_df.correctly_identified==False][selector])))\n",
    "    print(\"---\" + selector + \" ~ KL(False | Correct): {}\"\\\n",
    "        .format(KL(distribution_df[distribution_df.correctly_identified==False][selector], distribution_df[distribution_df.correctly_identified==True][selector])))\n",
    "\n",
    "####### Evaluate Model's Predictions on Drug Groups\n",
    "distribution_df['sgroup'] = distribution_df.index.map(lambda x: groups_dict[x[0]])\n",
    "distribution_df['ogroup'] =distribution_df.index.map(lambda x: groups_dict[x[1]])\n",
    "\n",
    "# Print (ordered) Accuracy and total samples for each category (when at least one drug on the interaction belongs to a category)\n",
    "group_names = ['approved', 'investigational', 'experimental', 'illicit', 'vet_approved', 'withdrawn', 'nutraceutical']\n",
    "\n",
    "for name in group_names:\n",
    "    temp_df = distribution_df[(distribution_df.sgroup==name) | (distribution_df.ogroup==name)]\n",
    "    accuracy = temp_df[temp_df.correctly_identified==True].shape[0] / temp_df.shape[0]\n",
    "    neg_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==0)].shape[0]\n",
    "    positive_samples = temp_df[((temp_df.sgroup==name) | (temp_df.ogroup==name)) & (temp_df.is_positive==1)].shape[0]\n",
    "    neg_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==0)].shape[0] / neg_samples\n",
    "    pos_acc =  temp_df[(temp_df.correctly_identified==True) & (temp_df.is_positive==1)].shape[0] / positive_samples\n",
    "    print(\"Total {} Drugs in Test Samples:  {} --- Model Accuracy {}\".format(name, temp_df.shape[0], accuracy))\n",
    "    print(\"\\t\\t{} Drugs in Positive Interactions: {}    Accuracy:{}\".format(name, positive_samples, pos_acc))\n",
    "    print(\"\\t\\t{} Drugs in Negative Interactions: {}    Accuracy:{}\".format(name, neg_samples, neg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Accuracy for pairs with min_degree < N\n",
    "N = distribution_df.max_degree.max()\n",
    "print(\"Hybrid Experiment Max Degree: {}\".format(N))\n",
    "hybrid_min_degree_recall_list = []\n",
    "hybrid_min_degree_accuracy_list = []\n",
    "for n in tqdm(range(1,N+1)):\n",
    "    lddf = distribution_df[distribution_df.min_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    hybrid_min_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    hybrid_min_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "hybrid_mean_degree_recall_list = []\n",
    "hybrid_mean_degree_accuracy_list = []\n",
    "for n in tqdm(range(6,N+1)):\n",
    "    lddf = distribution_df[distribution_df.mean_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    hybrid_mean_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    hybrid_mean_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "hybrid_max_degree_recall_list = []\n",
    "hybrid_max_degree_accuracy_list = []\n",
    "for n in tqdm(range(11,N+1)):\n",
    "    lddf = distribution_df[distribution_df.max_degree <= n] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    hybrid_max_degree_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    hybrid_max_degree_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "C = distribution_df.core_diff.max()\n",
    "hybrid_core_diff_recall_list = []\n",
    "hybrid_core_diff_accuracy_list = []\n",
    "print(\"Hybrid Experiment Max Core Difference: {}\".format(C))\n",
    "for c in tqdm(range(0,C+1)):\n",
    "    lddf = distribution_df[distribution_df.core_diff <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    hybrid_core_diff_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    hybrid_core_diff_accuracy_list.append(100.0*correct/(correct + false))\n",
    "\n",
    "B = distribution_df.b_centrality.max()\n",
    "hybrid_b_centrality_recall_list = []\n",
    "hybrid_b_centrality_accuracy_list = []\n",
    "print(\"Hybrid Experiment Max Core Difference: {}\".format(C))\n",
    "for c in tqdm(np.arange(0.01,B+0.1, 0.1)):\n",
    "    lddf = distribution_df[distribution_df.b_centrality <= c] #low degree df\n",
    "    total_samples, correct, false, pos, neg = lddf.shape[0], lddf[lddf.correctly_identified == True].shape[0], lddf[lddf.correctly_identified == False].shape[0], lddf[lddf.is_positive == 1].shape[0], lddf[lddf.is_positive == 0].shape[0]\n",
    "    hybrid_b_centrality_recall_list.append(100.0*lddf[(lddf.is_positive == 1) & (lddf.correctly_identified == True)].shape[0] / lddf[lddf.is_positive == 1].shape[0])\n",
    "    hybrid_b_centrality_accuracy_list.append(100.0*correct/(correct + false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsPath = \"result_plots/large_drugbank_results/cross_experiment/groupD_\"   ### CHANGE THIS FOR EVERY EXPERIMENT\n",
    "SL = 500\n",
    "\n",
    "#### Plot recall from experiments (Min Degree)\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_min_degree_recall_list[:SL],'Node2Vec': node2vec_min_degree_recall_list[:SL], 'Hybrid': hybrid_min_degree_recall_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Min Degree')\n",
    "sns_plot.set(xlabel='Minimum Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_minDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_min_degree_recall_list,'Node2Vec': node2vec_min_degree_recall_list, 'Hybrid': hybrid_min_degree_recall_list})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Min Degree')\n",
    "sns_plot.set(xlabel='Minimum Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_minDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot Accuracy lines from experiments (Min Degree)\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_min_degree_accuracy_list[:SL],'Node2Vec': node2vec_min_degree_accuracy_list[:SL], 'Hybrid': hybrid_min_degree_accuracy_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Min Degree')\n",
    "sns_plot.set(xlabel='Minimum Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_minDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_min_degree_accuracy_list,'Node2Vec': node2vec_min_degree_accuracy_list, 'Hybrid': hybrid_min_degree_accuracy_list})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Min Degree')\n",
    "sns_plot.set(xlabel='Minimum Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_minDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot recall from experiments (Mean Degree)\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_mean_degree_recall_list[:SL],'Node2Vec': node2vec_mean_degree_recall_list[:SL], 'Hybrid': hybrid_mean_degree_recall_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Average Degree')\n",
    "sns_plot.set(xlabel='Average Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_avgDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_mean_degree_recall_list,'Node2Vec': node2vec_mean_degree_recall_list, 'Hybrid': hybrid_mean_degree_recall_list})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Average Degree')\n",
    "sns_plot.set(xlabel='Average Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_avgDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot Accuracy lines from experiments (Mean Degree)\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_mean_degree_accuracy_list[:SL],'Node2Vec': node2vec_mean_degree_accuracy_list[:SL], 'Hybrid': hybrid_mean_degree_accuracy_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Average Degree')\n",
    "sns_plot.set(xlabel='Average Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_avgDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_mean_degree_accuracy_list,'Node2Vec': node2vec_mean_degree_accuracy_list, 'Hybrid': hybrid_mean_degree_accuracy_list})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Average Degree')\n",
    "sns_plot.set(xlabel='Average Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_avgDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot recall from experiments (Max Degree)\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_max_degree_recall_list[:SL],'Node2Vec': node2vec_max_degree_recall_list[:SL], 'Hybrid': hybrid_max_degree_recall_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Max Degree')\n",
    "sns_plot.set(xlabel='Maximum Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_maxDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_max_degree_recall_list,'Node2Vec': node2vec_max_degree_recall_list, 'Hybrid': hybrid_max_degree_recall_list})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Max Degree')\n",
    "sns_plot.set(xlabel='Maximum Degree of a Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_maxDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot Accuracy lines from experiments (Max Degree)\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_max_degree_accuracy_list[:SL],'Node2Vec': node2vec_max_degree_accuracy_list[:SL], 'Hybrid': hybrid_max_degree_accuracy_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Max Degree')\n",
    "sns_plot.set(xlabel='Maximum Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_maxDegree_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_max_degree_accuracy_list,'Node2Vec': node2vec_max_degree_accuracy_list, 'Hybrid': hybrid_max_degree_accuracy_list})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Max Degree')\n",
    "sns_plot.set(xlabel='Maximum Degree of a Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_maxDegree.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot recall from experiments (Core Diff)\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_core_diff_recall_list[:SL],'Node2Vec': node2vec_core_diff_recall_list[:SL], 'Hybrid': hybrid_core_diff_recall_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Core Difference')\n",
    "sns_plot.set(xlabel='Core Difference of Drugs inside Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_core_diff_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_core_diff_recall_list,'Node2Vec': node2vec_core_diff_recall_list, 'Hybrid': hybrid_core_diff_recall_list})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Core Difference')\n",
    "sns_plot.set(xlabel='Core Difference of Drugs inside Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_core_diff.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot Accuracy lines from experiments (Core Diff)\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_core_diff_accuracy_list[:SL],'Node2Vec': node2vec_core_diff_accuracy_list[:SL], 'Hybrid': hybrid_core_diff_accuracy_list[:SL]})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Core Difference')\n",
    "sns_plot.set(xlabel='Core Difference of Drugs inside Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_core_diff_short.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_core_diff_accuracy_list,'Node2Vec': node2vec_core_diff_accuracy_list, 'Hybrid': hybrid_core_diff_accuracy_list})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Core Difference')\n",
    "sns_plot.set(xlabel='Core Difference of Drugs inside Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_core_diff.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot recall from experiments (B Centrality)\n",
    "recall_df = pd.DataFrame({'Mol2Vec': mol2vec_b_centrality_recall_list,'Node2Vec': node2vec_b_centrality_recall_list, 'Hybrid': hybrid_b_centrality_recall_list})\n",
    "sns_plot = sns.lineplot(data=recall_df)\n",
    "sns_plot.set_title('Recall over Betweenness Centrality')\n",
    "sns_plot.set(xlabel='Betweenness Centrality of Drug Interaction', ylabel='Recall (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"recall_b_centrality.png\", dpi=1000)\n",
    "plt.clf()\n",
    "\n",
    "#### Plot Accuracy lines from experiments (Core Diff)\n",
    "acc_df = pd.DataFrame({'Mol2Vec': mol2vec_b_centrality_accuracy_list,'Node2Vec': node2vec_b_centrality_accuracy_list, 'Hybrid': hybrid_b_centrality_accuracy_list})\n",
    "sns_plot = sns.lineplot(data=acc_df)\n",
    "sns_plot.set_title('Accuracy over Core Betweenness Centrality')\n",
    "sns_plot.set(xlabel='Betweenness Centrality of Drug Interaction', ylabel='Accuracy (%)')\n",
    "sns_plot.figure.savefig(resultsPath + \"accuracy_b_centrality.png\", dpi=1000)\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96ae755eca9209db9567e962320baa15fadc8d28f6666618a4050aab2e6023e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf_rdkit_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
